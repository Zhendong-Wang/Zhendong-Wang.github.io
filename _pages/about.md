---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a first year PhD student in the department of Statistics & Data Science of the University of Texas, Austin. Before coming to UT, I completed my Master degree at Columbia University, majoring in Data Science. I received my Bachelor degree in Civil Engineering from Tongji University in China, during which I went to the University of California, Berkeley for one year exchange study. 

Currently, I am supervised by professor Mingyuan Zhou, at the University of Texas, Austin, and we focused a lot on Bayesian deep learning, and reinforcement learning. 

In Bayesian deep learning, we proposed a new probabilistic modeling framework for Thompson sampling (TS). Most of the recent methods rely on using global variable uncertainty to do exploration in TS, which will suffer from high computation cost. However, we started from a distinct point, using local variable uncertainty with semi-implicit structure to do posterior approximation. Our experiment shows TS guided by local uncertainty achieves the state-of-the-art performance while having low computational complexity. This paper is accepted by ICML 2020.

In Reinforcement Learning, we focused on improving the sample efficency of RL algorithms on continuous control tasks by involving better uncertainty estimation in policy and Q function (state-action return). We proposed Implicit Distributional Reinforcement Learning, which consists of a distributional critic, built on two deep generator networks (DGNs), and a semi-implicit actor (SIA), powered by a flexible policy distribution. We found that DGN and SIA help each other and achieved state-of-the-art performance. This paper is accepted by NeurIPS 2020. 

I am fasicinated by the mechanism and pratical impact of Reinforcement Learning, and going to do more research in RL, like Imitation Learning and meta-learning in RL. 


